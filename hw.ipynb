{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8d7840a",
   "metadata": {},
   "source": [
    "# HW1: Linear Regression\n",
    "\n",
    "Created by Kien X. Nguyen and Xi Peng, Sept 2023.\n",
    "\n",
    "## Questions and Answers [20 points]\n",
    "\n",
    "Answer following questions. \n",
    "\n",
    "### Q.1 True or False [3 points]\n",
    "\n",
    "Please choose True or False for each statement:\n",
    "\n",
    "(a) Forecast tomorrow’s Dow Jones Industrial (DJI) index is a regression task.\n",
    "\n",
    "True\n",
    "\n",
    "(b) In general, semi-supervised learning is more challenging than unsupervised learning.\n",
    "\n",
    "False\n",
    "\n",
    "(c) It is impossible to solve a classification task using unsupervised learning techniques.\n",
    "\n",
    "False\n",
    "\n",
    "(d) How to generate an image (or an audio sequence) is a reinforcement learning task.\n",
    "\n",
    "False\n",
    "\n",
    "(e) It is hard for machine learning model to know “I do now know”.\n",
    "\n",
    "False\n",
    "\n",
    "(f) The goal of life-long learning is to learn for a long time rather than addressing the catastrophic forgetting issue.\n",
    "\n",
    "False\n",
    "\n",
    "### Q.2 Gradient descent [4 points]\n",
    "\n",
    "Suppose we are using gradient descent to learn linear regression. The hypothesis is $h_{\\theta}(x)=\\theta_0+\\theta_1x$. The initial values are $\\theta_0=2$,$\\theta_1=1$, and the learning rate is $0.1$. Suppose we have one data exmaple $(5,5)$ and use only this data example to update the model:\n",
    "\n",
    "(a) After the first step update, what is $\\theta_0$ and $\\theta_1$, respectively?\n",
    "\n",
    "$x=5, \\alpha = 0.1$\n",
    "\n",
    "$h_{\\theta}(x)=2+1*5=7, \\text{actual} = 5, 7-5=2$\n",
    "\n",
    "$\\frac 1 2(h_{\\theta}(x)-y)^2 = \\frac 1 2(2)^2 = 2$\n",
    "\n",
    "Partial of $\\theta_0=\\theta_0+\\theta_1x-y$\n",
    "\n",
    "Partial of $\\theta_1=(\\theta_0+\\theta_1x-y)*x$\n",
    "\n",
    "(b) After the second step update, what is $\\theta_0$ and $\\theta_1$, respectively?\n",
    "\n",
    "### Q.3  [5 points]\n",
    "\n",
    "Consider the problem of predicting how well a student does in her second year of college/university, given how well she did in her first year. Specifcally, let x be equal to the number of \"A\" grades (including A-. A and A+ grades) that a student receives in their first year of college (freshmen year). We would like to predict the value of y, which we define as the number of \"A\" grades they get in their second year (sophomore year).\n",
    "Here each row is one training example. Recall that in linear regression, our hypothesis is $h_{\\theta}(x)=\\theta_0+\\theta_1x$, and we use m to denote the number of training examples.\n",
    "\n",
    "| x | y |\n",
    "|---|---|\n",
    "| 3 | 2 |\n",
    "| 1 | 2 |\n",
    "| 0 | 1 |\n",
    "| 4 | 3 |\n",
    "\n",
    "For the training set given above (note that this training set may also be referenced in other questions in this quiz), what is the value of m? (which should be a number between 0 and 10).\n",
    "\n",
    "### Q.4  [4 points]\n",
    "\n",
    "For this question, assume that we are using the training set from Q.3. Recall our definition of the cost function was $J\\left(\\theta_{0}, \\theta_{1}\\right)=\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}$. What is $J(0,1)$ (Simplify fractions to decimals when entering answer, and '.' as the decimal delimiter e.g., 1.5).\n",
    "\n",
    "\n",
    "### Q.5  [4 points]\n",
    "\n",
    "In the given figure, the cost function $J\\left(\\theta_{0}, \\theta_{1}\\right)$ has been plotted against $\\theta_{0}$ and $\\theta_{1}$, as shown in 'Plot 2'. The contour plot for the same cost function is given in 'Plot 1'. Based on the figure, choose the correct options (check all that apply).\n",
    "\n",
    "<img src=\"figure/loss_landscape.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "(a) Point P (The global minimum of plot 2) corresponds to point C of Plot 1.\n",
    "\n",
    "(b) Point P (the global minimum of plot 2) corresponds to point A of Plot 1.\n",
    "\n",
    "(c) If we start from point B, gradient descent with a well-chosen learning rate will eventually help us reach at or near point A, as the value of cost function $J\\left(\\theta_{0}, \\theta_{1}\\right)$ is minimum at A.\n",
    "\n",
    "(d) If we start from point B, gradient descent with a well-chosen learning rate will eventually help us reach at or near point C, as the value of cost function $J\\left(\\theta_{0}, \\theta_{1}\\right)$ is minimum at point C.\n",
    "\n",
    "(e) If we start from point B, gradient descent with a well-chosen learning rate will eventually help us reach at or near point A, as the value of cost function $J\\left(\\theta_{0}, \\theta_{1}\\right)$ is maximum at point A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3760e2ae",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Linear Regression [25 points]\n",
    "\n",
    "In this section, we are going to implement linear regression from scratch using NumPy, a popular scientic Python package.\n",
    "\n",
    "First, we need to import some necessary Python packages. NumPy, Matplotlib and Scikit-learn are three of the most popular packages in Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c11489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df7bdc2",
   "metadata": {},
   "source": [
    "The ```create_dataset()``` method allows us to create a customizable synthetic dataset. ```n_samples``` means the number of data points in the dataset, ```n_features``` means the dimension of the input ```X```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876804f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(n_samples=20, n_features=1, noise=0.4):\n",
    "    X, y = make_regression(n_samples=n_samples, n_features=n_features, noise=noise)\n",
    "    return X, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a920ba19",
   "metadata": {},
   "source": [
    "Next we call the method to create our dataset and assign the return values to ```X```, the inputs, and ```y```, the ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d11abf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_dataset(n_samples=100, n_features=1, noise=10.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805bc79d",
   "metadata": {},
   "source": [
    "Visualize the dataset by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c40607",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec96261",
   "metadata": {},
   "source": [
    "### 1.1 The ```LinearReg``` Class [20 points]\n",
    "\n",
    "Here, we implement the class for our Linear Regression algorithm. Some parts of the code are missing and needed to be filled in. Hints are given throughout the missing codes. Please refer to what has been covered in slides to complete the codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d013aeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearReg:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, epochs=10000, lmbda=0.0):\n",
    "        self.epochs = epochs # number of epochs to train\n",
    "        self.lr = learning_rate # learning rate - how fast the model should be updated\n",
    "        self.w = None # the weights or coefficients\n",
    "        self.b = None # the bias term\n",
    "        self.lmbda = lmbda # regularization coefficient\n",
    "        self.cost_list = [] # cost list for plotting\n",
    "        \n",
    "    def __initial_params(self, shape):\n",
    "        #initialize weigth and bias as zeros\n",
    "        \n",
    "        #### START CODE HERE (~2 line) ####\n",
    "\n",
    "        \n",
    "        #### END CODE HERE ####\n",
    "        \n",
    "    def __predictions(self, X):\n",
    "        return np.dot(X, self.w) + self.b\n",
    "    \n",
    "    def __calculate_cost(self, error):\n",
    "        #Calculate cost\n",
    "        # Return: The mean squared error (MSE) of a given error vector. \n",
    "        # MSE = (1/2n) * Σ(y_actual - y_pred)^2  + regularization term\n",
    "        # Where: n is the total number of data points.  Σ represents the summation symbol, indicating that you need to sum up the squared differences for all data points.\n",
    "\n",
    "        ### START CODE HERE ### (≈ 1 lines of code)        \n",
    "\n",
    "        ### END CODE HERE ###      \n",
    "\n",
    "    def __gradient_descent(self, X, y, y_pred):\n",
    "        # difference between prediction and actual\n",
    "        error = y_pred - y\n",
    "        \n",
    "        # calculate cost and append them to list\n",
    "        cost = self.__calculate_cost(error)\n",
    "        self.cost_list.append(cost)\n",
    "\n",
    "        # Calculate gradients\n",
    "        # Return:\n",
    "        # dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "        #    -- make sure to account for the regularization term!\n",
    "        # db -- gradient of the loss with respect to b, thus same shape as b\n",
    "\n",
    "        ### START CODE HERE ### (≈ 3 lines of code)        \n",
    "\n",
    "        \n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    def __update_parameters(self, dw, db):\n",
    "        #update weight and bias with gradients\n",
    "        self.w -= self.lr * dw\n",
    "        self.b -= self.lr * db\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"fits the model\"\"\"\n",
    "        self.__initial_params(X.shape[1])\n",
    "        for _ in range(self.epochs):\n",
    "            y_pred = self.__predictions(X)\n",
    "            dw, db,  = self.__gradient_descent(X, y, y_pred)\n",
    "            self.__update_parameters(dw, db)\n",
    "        return True\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.__predictions(X)\n",
    "    \n",
    "    def calculate_rmse(self, y_real, y_pred):\n",
    "        \"\"\"returns root mean square error\"\"\"\n",
    "        \n",
    "        ### START CODE HERE ### (≈ 1 lines of code)        \n",
    "\n",
    "        ### END CODE HERE ###       \n",
    "\n",
    "    def get_params(self):\n",
    "        return self.w, self.b\n",
    "    \n",
    "    def plot_cost(self):\n",
    "        \"\"\"plots the progress of cost on each iteration\"\"\"\n",
    "        plt.title('Cost Function J')\n",
    "        plt.xlabel('No. of iterations')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.plot(self.cost_list)\n",
    "        plt.show()\n",
    "        \n",
    "    def calculate_r2(self, X, y):\n",
    "        \"\"\"returns r2\"\"\"\n",
    "        sum_squares = 0\n",
    "        sum_residuals = 0\n",
    "        y_mean = np.mean(y)\n",
    "        for i in range(X.shape[0]):\n",
    "            y_pred = self.__predictions(X[i])\n",
    "            sum_squares += (y[i] - y_mean) ** 2\n",
    "            sum_residuals += (y[i] - y_pred) ** 2\n",
    "        score = 1- (sum_residuals / sum_squares)\n",
    "        return score\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b4e38b",
   "metadata": {},
   "source": [
    "Finally, instantiate an instance of the ```LinearReg``` class, train it on the dataset via ```fit()```, and test its final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f630b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearReg(learning_rate=0.01,epochs=10000)\n",
    "reg.fit(X, y) # fit/train the model\n",
    "\n",
    "# get predictions\n",
    "y_pred = reg.predict(X)\n",
    "\n",
    "# plot\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1b1e7b",
   "metadata": {},
   "source": [
    "### 1.2 Compare to Built-in Methods [5 points]\n",
    "\n",
    "In this section, you will call the Linear Regression algorithm built into ```scikit-learn``` and compare the one we built to theirs on the dataset we just created in this assignment. Please refer to their official [website](https://scikit-learn.org/stable/) and documentation for the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae537a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================= YOUR CODE HERE ===========================\n",
    "\n",
    "\n",
    "\n",
    "# ==================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ae186d",
   "metadata": {},
   "source": [
    "## 2 Regularized Linear Regression and Polynomial Regression [55 points]\n",
    "\n",
    "In this part of the exercise, you will replicate and expand upon the prototype data analysis problem in the lecture on Polynomial Regression.\n",
    "\n",
    "### 2.1 Regularization [10 points]\n",
    "\n",
    "We create the data by sampling the sinusoidal function $f(x)=sin(2\\pi x), x \\in [0,1]$. Write the code to generate a dataset consisting of $m=200$ by evaluating the function $f(x)$ at $m$ uniformly sapced points in $x$ and adding I.I.D. white Gaussian noise with deviation $\\sigma=0.1$. For instance: $$x_i = \\frac{i}{m}, i=0,1,2,\\cdots,m-1, y_i = f(x_i)+\\epsilon_i, \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "\n",
    "Once generate the dataset, we can plot it out. You should get a similar figure like this:\n",
    "\n",
    "![c](figure/dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7a8127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_poly_dataset(n_samples, mu=0, sigma=0.1):\n",
    "    # mu is the mean of the normal distribution that we are going to sample the noise from\n",
    "    # sigma is the std of the normal distribution that we are going to sample the noise from\n",
    "    m = n_samples\n",
    "    X_data = np.array([(i)/(m) for i in range(m)])\n",
    "\n",
    "    # Calculate Y\n",
    "    # Plot the dataset\n",
    "    # ======================= YOUR CODE HERE ===========================\n",
    "    # create an array of normally distributed data (noise) the same size as X data\n",
    "    noise = \n",
    "    # calculate the sine data from X data and add the noise\n",
    "    Y_data = \n",
    "    return X_data, Y_data\n",
    "\n",
    "# create and plot the dataset\n",
    "# ======================= YOUR CODE HERE ===========================\n",
    "\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1363a4e7",
   "metadata": {},
   "source": [
    "Next, use the above ```LinReg``` class to train multiple regressors with different regularization coefficients ```lambda```. Then, plot the results of training and testing errors against the coefficients, like the example in the course slide.\n",
    "\n",
    "Your plot does not have to resemble exactly the plot below. As long as it conveys the overfitting and underfitting phenomena, you will get full credit for this part.\n",
    "\n",
    "<img src=\"figure/regularization.png\" alt=\"regularization plot\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970badc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================= YOUR CODE HERE ===========================\n",
    "\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6f7ccc",
   "metadata": {},
   "source": [
    "### 2.2 Normal equation [20 points]\n",
    "\n",
    "Normal equation is the closed-form solution to linear regression, formulated as:\n",
    "\n",
    "$$ \\theta = \\left( X^T X\\right)^{-1} X^T\\vec{y}$$\n",
    "\n",
    "Using this formula does not require any feature scaling, and you will get an exact solution in one calculation: there is no “loop until convergence” like in gradient descent. \n",
    "\n",
    "Hint: Before implement normal_equation function, following example shows how to convert array data to a matrix with a shape of $m \\times 1$, and generate polynomial features matrix $[1, X, X^2, X^3, \\cdots]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca18f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an example that converts X to a column matrix of 5 x 1 and generates polynomial [1 X X^2 X^3]\n",
    "# Use this hint to implement normal_equation()\n",
    "X_col = X_data[:5].reshape(5,1)\n",
    "X_poly = np.power(X_col, np.arange(4))\n",
    "print(X_col)\n",
    "print(X_poly)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54102feb",
   "metadata": {},
   "source": [
    "Complete the code for `normal_equation()` below which uses normal equation to estimate $\\theta$ based on $X$ and $Y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fbe85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_equation(X, Y, n):\n",
    "    \"\"\"\n",
    "    Computes the closed-form solution to linear regression using the normal equations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset of shape (m, ).\n",
    "    \n",
    "    Y : array_like\n",
    "        The value at each data point. A vector of shape (m, ).\n",
    "        \n",
    "    n : the order of polynomial regression model\n",
    "        Remember the number of features will be n+1.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    theta : array_like\n",
    "        Estimated polynomial regression parameters. A vector of shape (n+1, ).\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Complete the code to compute the closed form solution to linear\n",
    "    regression and put the result in theta.\n",
    "    \n",
    "    Hint\n",
    "    ----\n",
    "    Look up the function `np.linalg.pinv` for computing matrix inverse.\n",
    "    \"\"\"\n",
    "    m = X.size\n",
    "    theta = np.zeros(n+1)\n",
    "    \n",
    "    # ===================== YOUR CODE HERE ============================\n",
    "    X_col = \n",
    "    X_poly = \n",
    "    Y_col = \n",
    "    \n",
    "    theta = \n",
    "        \n",
    "    # =================================================================\n",
    "    return theta.flatten()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d1d999",
   "metadata": {},
   "source": [
    "Complete the code for `polynomial_deploy()` below which uses estimated $\\theta$ to predict $Y$ given $X$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e079b756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_deploy(X, theta):\n",
    "    \"\"\"\n",
    "    Computes the polynomial regression prediction for data X.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The input data. A vector of shape (m, ).\n",
    "    \n",
    "    theta : array_like\n",
    "        Polynomial regression parameters. A vector of shape (n+1, ).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Y : array_like\n",
    "        Polynomial prediction. A vector of shape (m, ).\n",
    "    \n",
    "    \"\"\"\n",
    "    m = X.size\n",
    "    n = theta.size - 1\n",
    "    \n",
    "    # ===================== YOUR CODE HERE ============================\n",
    "    X_col = \n",
    "    X_poly = \n",
    "    \n",
    "    theta_col = \n",
    "    \n",
    "    Y = \n",
    "    # ===================== YOUR CODE HERE ============================\n",
    "    \n",
    "    return Y.flatten()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b75b4f",
   "metadata": {},
   "source": [
    "### 2.3 Polynomial fitting [15 points]\n",
    "\n",
    "Now repeat the polynomial fitting experiment by selecting a random subset of the dataset to train the model. The training set size is selected as: $m=\\{10, 50, 100, 200\\}$. The order of polynomials is selected as: $n=\\{0,1,2,3,9\\}$. So totally you will train $4\\times5=20$ models.\n",
    "\n",
    "For each model, report estimated model parameters, calculate the MSEs, and plot the regression model together with the data examples used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0388c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You MUST use LOOP to traverse the combinations of m and n\n",
    "#\n",
    "# Hint for debug:\n",
    "# A quick way to check if your implementation is correct is to use 'np.polyfit'\n",
    "# e.g. theta = np.polyfit(X_data, Y_data, n)\n",
    "# Check if 'theta' estimated by normal_equation() is the same as 'theta' \n",
    "\n",
    "for m in [10, 50, 100, 200]:\n",
    "    for n in [0, 1, 2, 3, 9]:\n",
    "        subset_index = np.random.randint(0, X_data.size, m)\n",
    "        \n",
    "        X_subset = X_data[subset_index]\n",
    "        Y_subset = Y_data[subset_index]\n",
    "        \n",
    "        # ===================== YOUR CODE HERE ============================\n",
    "        theta = \n",
    "        Y_predict = \n",
    "        MSE = \n",
    "                        \n",
    "        print('The order of polynomial: %d' % n)\n",
    "        print('Theta:')\n",
    "        print(theta)\n",
    "        print('MSE: %.8f' % MSE)\n",
    "        \n",
    "        # plot the polynomial curve\n",
    "        plt.figure()\n",
    "        plt.title('m=%d, n=%d, MSE=%.8f' % (m, n, MSE))\n",
    "        plt.plot(X_subset, Y_subset, 'b+')\n",
    "        X_plot = np.arange(0,1,0.01)\n",
    "        Y_plot = polynomial_deploy(X_plot, theta)\n",
    "        plt.plot(X_plot, Y_plot, 'r-')          \n",
    "        # =================================================================\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29800424",
   "metadata": {},
   "source": [
    "Next, implement a scenario where the training set size is fixed (your choice), fit the model on different polynomial degrees and plot them agains train and test errors, like the plot in the course slide.\n",
    "\n",
    "Again, your plot does not have to resemble exactly the plot below. As long as it conveys the overfitting and underfitting phenomena, you will get full credit for this part.\n",
    "\n",
    "<img src=\"figure/poly.png\" alt=\"regularization plot\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd29b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================= YOUR CODE HERE ===========================\n",
    "\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f74d66",
   "metadata": {},
   "source": [
    "### 2.4 Discussions [10 points]\n",
    "\n",
    "Based on the experimental results, talk about your observations. \n",
    "\n",
    "======================= YOUR DISCUSSION HERE ==========================\n",
    "\n",
    "Try to talk about following aspects:\n",
    "1. Given fixed m, when n increase, how about underfitting/overfitting and fitting error?  \n",
    "2. Given fixed n, when m increase, how about underfitting/overfitting and fitting error?\n",
    "3. Which model has the best performance? And why?\n",
    "4. Your conclusion.\n",
    "\n",
    "======================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd33ecc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
