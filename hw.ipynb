{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "#### Modified by Kien Nguyen, Oct 2023\n",
    "\n",
    "# Part 0 [40 points]\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### What do you need to do?\n",
    "#### a) Follow this Jupyter Notebook File (hw.ipynb) LINE by LINE;\n",
    "#### b) Implement your codes in the required functions;\n",
    "#### c) Answer all the questions and paste all required figures.\n",
    "\n",
    "### What do you need to submit?\n",
    "#### a) \"hw.ipynb\" (runable with your codes and answers).\n",
    "#### b) \"./figure/\" (contains the figures of your answers to part 2 if any)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Questions and Answers [5 points each for 2.1-2.6, 10 points for 2.7]\n",
    "\n",
    "Answer following questions. For mathmetical inputs, if you are not familiar with Latex, you can write the proof on paper, scan, and paste the copies right after the questions.\n",
    "\n",
    "### 2.1 Hypothesis\n",
    "\n",
    "a) When train logistic regression model,\n",
    "\n",
    "If $y=1$ we want $h_{\\theta}(x)$:\n",
    "\n",
    "If $y=0$ we want $h_{\\theta}(x)$:\n",
    "\n",
    "b) When apply a trained logistic regression model,\n",
    "\n",
    "We pridict $y=1$ when $h_{\\theta}(x)$:\n",
    "\n",
    "We pridict $y=0$ when $h_{\\theta}(x)$:\n",
    "\n",
    "c) When train a SVM model,\n",
    "\n",
    "If $y=1$ we want $h_{\\theta}(x)$:\n",
    "\n",
    "If $y=0$ we want $h_{\\theta}(x)$:\n",
    "\n",
    "d) When apply a trained SVM model,\n",
    "\n",
    "We pridict $y=1$ when $h_{\\theta}(x)$:\n",
    "\n",
    "We pridict $y=0$ when $h_{\\theta}(x)$:\n",
    "\n",
    "\n",
    "### 2.2 Kernel and Margin\n",
    "\n",
    "a) In one or two sentences, describe the benefit of using the kernel trick.\n",
    "\n",
    "b) The concept of margin is essential in SVM. Describe why a large margin separator is desirable for classification.\n",
    "\n",
    "\n",
    "### 2.3 Kernel SVM\n",
    "\n",
    "a) List the hyperparameter(s) in learning SVMs with Gaussion kernel.\n",
    "\n",
    "b) Explain how the hyperparameter(s) affect the bias and variance trade-off.  \n",
    "\n",
    "\n",
    "### 2.4 Decision Boundary\n",
    "\n",
    "Consider the dataset in the following figure. Answer the questions.\n",
    "\n",
    "<img src=\"figure/toyexample.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "a) Draw the desicion boundary on the graph.\n",
    "\n",
    "b) What is the size of the margin? (Show the calculation process)\n",
    "\n",
    "c) Cirle all the support vectors on the graph.\n",
    "\n",
    "\n",
    "### 2.5 Large Margin\n",
    "\n",
    "The SVM optimization we used is:\n",
    "\n",
    "![](figure/svmobjective.png)\n",
    "\n",
    "<img src=\"figure/toyexample2.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "where $p^{(i)}$ is the (signed) projection of $x^{(i)}$ onto $\\theta$. Consider the training set above. At the optimal value of $\\theta$, what is the value of $\\|\\theta\\|$? (Show the calculation process)\n",
    "\n",
    "\n",
    "### 2.6 Plot\n",
    "\n",
    "Considering the model selection setup, plot two figures which show how the values of ACCURACY change with respect to the degree of polynomial $d$ and the regularization hyperparameter $\\lambda$, respectively. In each figure, you should plot two curves: one for accuracy on the training set and the other for accuracy on the testing set.\n",
    "\n",
    "### 2.7 Proof\n",
    "\n",
    "Prove that with different means $\\mu_i$ and similar covariances $\\Sigma$, 2-class Gaussian Discriminant Analysis (GDA) is equivalent to Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part I [110 points]\n",
    "## 1 - Packages ##\n",
    "\n",
    "First, let's run the cell below to import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [h5py](http://www.h5py.org) is a common package to interact with a dataset that is stored on an H5 file.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
    "- [PIL](http://www.pythonware.com/products/pil/) and [scipy](https://www.scipy.org/) are used here to test your model with your own picture at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from lr_utils import load_dataset\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Problem Overview ##\n",
    "\n",
    "**Problem Statement**: You are given a dataset (\"data.h5\") containing:\n",
    "    - a training set of m_train images labeled as cat (y=1) or non-cat (y=0)\n",
    "    - a test set of m_test images labeled as cat or non-cat\n",
    "    - each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px).\n",
    "\n",
    "You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.\n",
    "\n",
    "Let's get more familiar with the dataset. Load the data by running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data (cat/non-cat)\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added \"_orig\" at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don't need any preprocessing).\n",
    "\n",
    "Each line of your train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the `index` value and re-run to see other images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [1], it's a 'cat' picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20040fe2f28>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 24\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "print (\"y = \" + str(train_set_y[:,index]) + \", it's a '\" \n",
    "       + classes[np.squeeze(train_set_y[:,index])].decode(\"utf-8\") +  \"' picture.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many software bugs in deep learning come from having matrix/vector dimensions that don't fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs. \n",
    "\n",
    "**Exercise:** Find the values for:\n",
    "    - m_train (number of training examples)\n",
    "    - m_test (number of test examples)\n",
    "    - num_px (= height = width of a training image)\n",
    "Remember that `train_set_x_orig` is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access `m_train` by writing `train_set_x_orig.shape[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 209\n",
      "Number of testing examples: m_test = 50\n",
      "Height/Width of each image: num_px = 64\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_set_x shape: (209, 64, 64, 3)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x shape: (50, 64, 64, 3)\n",
      "test_set_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (≈ 3 lines of code)\n",
    "m_train = train_set_x_orig.shape[0]\n",
    "m_test = test_set_x_orig.shape[0]\n",
    "num_px = train_set_x_orig.shape[1]\n",
    "\n",
    "### END CODE HERE ###\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output for m_train, m_test and num_px**: \n",
    "<table style=\"width:15%\">\n",
    "  <tr>\n",
    "    <td>m_train</td>\n",
    "    <td> 209 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>m_test</td>\n",
    "    <td> 50 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>num_px</td>\n",
    "    <td> 64 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px $*$ num_px $*$ 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.\n",
    "\n",
    "**Exercise:** Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num\\_px $*$ num\\_px $*$ 3, 1).\n",
    "\n",
    "A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b$*$c$*$d, a) is to use: \n",
    "```python\n",
    "X_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x_flatten shape: (12288, 209)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x_flatten shape: (12288, 50)\n",
      "test_set_y shape: (1, 50)\n",
      "sanity check after reshaping: [17 31 56 22 33]\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples\n",
    "\n",
    "### START CODE HERE ### (≈ 2 lines of code)\n",
    "train_set_x_flatten = train_set_x_orig.reshape(m_train, -1).T\n",
    "test_set_x_flatten = test_set_x_orig.reshape(m_test, -1).T\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
    "print (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:35%\">\n",
    "  <tr>\n",
    "    <td>train_set_x_flatten shape</td>\n",
    "    <td> (12288, 209)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>train_set_y shape</td>\n",
    "    <td>(1, 209)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>test_set_x_flatten shape</td>\n",
    "    <td>(12288, 50)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>test_set_y shape</td>\n",
    "    <td>(1, 50)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>sanity check after reshaping</td>\n",
    "  <td>[17 31 56 22 33]</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\n",
    "\n",
    "One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).\n",
    "\n",
    "<!-- During the training of your model, you're going to multiply weights and add biases to some initial inputs in order to observe neuron activations. Then you backpropogate with the gradients to train the model. But, it is extremely important for each feature to have a similar range such that our gradients don't explode. You will see that more in detail later in the lectures. !--> \n",
    "\n",
    "Let's standardize our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = train_set_x_flatten / 255.\n",
    "test_set_x = test_set_x_flatten / 255.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "What you need to remember:\n",
    "\n",
    "Common steps for pre-processing a new dataset are:\n",
    "- Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, ...)\n",
    "- Reshape the datasets such that each example is now a vector of size (num_px \\* num_px \\* 3, 1)\n",
    "- \"Standardize\" the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Knowedge Background ##\n",
    "\n",
    "It's time to design a simple algorithm to distinguish cat images from non-cat images.\n",
    "\n",
    "You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why **Logistic Regression is actually a very simple Neural Network!**\n",
    "\n",
    "<img src=\"images/LogReg_kiank.png\" style=\"width:650px;height:400px;\">\n",
    "\n",
    "**Mathematical expression of the algorithm**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n",
    "\n",
    "**Key steps**:\n",
    "In this exercise, you will carry out the following steps: \n",
    "    - Initialize the parameters of the model\n",
    "    - Learn the parameters for the model by minimizing the cost  \n",
    "    - Use the learned parameters to make predictions (on the test set)\n",
    "    - Analyse the results and conclude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Model Implementation ## \n",
    "\n",
    "The main steps for building a Neural Network are:\n",
    "1. Define the model structure (such as number of input features) \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Calculate current loss (forward propagation)\n",
    "    - Calculate current gradient (backward propagation)\n",
    "    - Update parameters (gradient descent)\n",
    "\n",
    "You often build 1-3 separately and integrate them into one function we call `model()`.\n",
    "\n",
    "### 4.1 - Helper functions\n",
    "\n",
    "**Exercise**: Using your code from \"Python Basics\", implement `sigmoid()`. As you've seen in the figure above, you need to compute $sigmoid( w^T x + b)$ to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(0) = 0.5\n",
      "sigmoid(9.2) = 0.999898970806\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoid(0) = \" + str(sigmoid(0)))\n",
    "print (\"sigmoid(9.2) = \" + str(sigmoid(9.2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:20%\">\n",
    "  <tr>\n",
    "    <td>sigmoid(0)</td>\n",
    "    <td> 0.5</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>sigmoid(9.2)</td>\n",
    "    <td> 0.999898970806 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Initializing parameters\n",
    "\n",
    "**Exercise:** Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don't know what numpy function to use, look up np.zeros() in the Numpy library's documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_with_zeros\n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    w = np.zeros((dim, 1))\n",
    "    b = 0\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[ 0.]\n",
      " [ 0.]]\n",
      "b = 0\n"
     ]
    }
   ],
   "source": [
    "dim = 2\n",
    "w, b = initialize_with_zeros(dim)\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "\n",
    "<table style=\"width:15%\">\n",
    "    <tr>\n",
    "        <td>  w   </td>\n",
    "        <td> [[ 0.]\n",
    " [ 0.]] </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>  b   </td>\n",
    "        <td> 0 </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "For image inputs, w will be of shape (num_px $\\times$ num_px $\\times$ 3, 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Forward and backward propagation\n",
    "\n",
    "Now that your parameters are initialized, you can do the \"forward\" and \"backward\" propagation steps for learning the parameters.\n",
    "\n",
    "**Exercise:** Implement a function `propagate()` that computes the cost function and its gradient.\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "Forward Propagation:\n",
    "- You get X\n",
    "- You compute $A = \\sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: propagate\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A = sigmoid(np.dot(w.T, X) + b)                                    # compute activation\n",
    "    cost = -1/m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))     # compute cost\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    dw = 1/m * np.dot(X, (A - Y).T)\n",
    "    db = 1/m * np.sum(A - Y)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw = [[ 0.99993216]\n",
      " [ 1.99980262]]\n",
      "db = 0.499935230625\n",
      "cost = 6.00006477319\n"
     ]
    }
   ],
   "source": [
    "w, b, X, Y = np.array([[1], [2]]), 2, np.array([[1,2], [3,4]]), np.array([[1, 0]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:50%\">\n",
    "    <tr>\n",
    "        <td>  dw   </td>\n",
    "        <td> [[ 0.99993216]\n",
    " [ 1.99980262]]</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>   db   </td>\n",
    "        <td> 0.499935230625 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>   cost   </td>\n",
    "        <td> 6.000064773192205</td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Optimization\n",
    "- You have initialized your parameters.\n",
    "- You are also able to compute a cost function and its gradient.\n",
    "- Now, you want to update the parameters using gradient descent.\n",
    "\n",
    "**Exercise:** Write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        ### START CODE HERE ### \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[ 0.1124579 ]\n",
      " [ 0.23106775]]\n",
      "b = 1.55930492484\n",
      "dw = [[ 0.90158428]\n",
      " [ 1.76250842]]\n",
      "db = 0.430462071679\n"
     ]
    }
   ],
   "source": [
    "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:50%\">\n",
    "    <tr>\n",
    "       <td>  w  </td>\n",
    "       <td>[[ 0.1124579 ]\n",
    " [ 0.23106775]] </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td>  b  </td>\n",
    "       <td> 1.55930492484 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td>  dw  </td>\n",
    "       <td> [[ 0.90158428] \n",
    " [ 1.76250842]] </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td>  db  </td>\n",
    "       <td> 0.430462071679 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the `predict()` function. There is two steps to computing predictions:\n",
    "\n",
    "1. Calculate $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "\n",
    "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector `Y_prediction`. If you wish, you can use an `if`/`else` statement in a `for` loop (though there is also a way to vectorize this). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        # Convert probabilities a[0,i] to actual predictions p[0,i]\n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        if A[0, i] > 0.5:\n",
    "            Y_prediction[0, i] = 1\n",
    "        else:\n",
    "            Y_prediction[0, i] = 0\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = [[ 1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"predictions = \" + str(predict(w, b, X)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:30%\">\n",
    "    <tr>\n",
    "         <td>\n",
    "             predictions\n",
    "         </td>\n",
    "          <td>\n",
    "            [[ 1.  1.]]\n",
    "         </td>  \n",
    "   </tr>\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "What to remember:\n",
    "You've implemented several functions that:\n",
    "- Initialize (w,b)\n",
    "- Optimize the loss iteratively to learn parameters (w,b):\n",
    "    - computing the cost and its gradient \n",
    "    - updating the parameters using gradient descent\n",
    "- Use the learned (w,b) to predict the labels for a given set of examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Model Training and Testing ##\n",
    "\n",
    "You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\n",
    "\n",
    "**Exercise:** Implement the model function. Use the following notation:\n",
    "    - Y_prediction for your predictions on the test set\n",
    "    - Y_prediction_train for your predictions on the train set\n",
    "    - w, costs, grads for the outputs of optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # initialize parameters with zeros (≈ 1 line of code)\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w, b = parameters[\"w\"], parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.584508\n",
      "Cost after iteration 200: 0.466949\n",
      "Cost after iteration 300: 0.376007\n",
      "Cost after iteration 400: 0.331463\n",
      "Cost after iteration 500: 0.303273\n",
      "Cost after iteration 600: 0.279880\n",
      "Cost after iteration 700: 0.260042\n",
      "Cost after iteration 800: 0.242941\n",
      "Cost after iteration 900: 0.228004\n",
      "Cost after iteration 1000: 0.214820\n",
      "Cost after iteration 1100: 0.203078\n",
      "Cost after iteration 1200: 0.192544\n",
      "Cost after iteration 1300: 0.183033\n",
      "Cost after iteration 1400: 0.174399\n",
      "Cost after iteration 1500: 0.166521\n",
      "Cost after iteration 1600: 0.159305\n",
      "Cost after iteration 1700: 0.152667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 1800: 0.146542\n",
      "Cost after iteration 1900: 0.140872\n",
      "train accuracy: 99.04306220095694 %\n",
      "test accuracy: 70.0 %\n"
     ]
    }
   ],
   "source": [
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, \n",
    "          learning_rate = 0.005, print_cost = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:40%\"> \n",
    "    <tr>\n",
    "        <td> Train Accuracy  </td> \n",
    "        <td> 99.04306220095694 % </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Test Accuracy </td> \n",
    "        <td> 70.0 % </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test error is 68%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the cost function and the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x200410204a8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot learning curve (with costs)\n",
    "costs = np.squeeze(d['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**:\n",
    "You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Further analysis ##\n",
    "\n",
    "Congratulations on building your first image classification model. Let's analyze it further, and examine possible choices for the learning rate $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choice of learning rate ####\n",
    "\n",
    "**Reminder**:\n",
    "In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate $\\alpha$  determines how rapidly we update the parameters. If the learning rate is too large we may \"overshoot\" the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That's why it is crucial to use a well-tuned learning rate.\n",
    "\n",
    "Let's compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the `learning_rates` variable to contain, and see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate is: 0.01\n",
      "train accuracy: 99.52153110047847 %\n",
      "test accuracy: 68.0 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "learning rate is: 0.001\n",
      "train accuracy: 88.99521531100478 %\n",
      "test accuracy: 64.0 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "learning rate is: 0.0001\n",
      "train accuracy: 68.42105263157895 %\n",
      "test accuracy: 36.0 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x200431109e8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "models = {}\n",
    "for i in learning_rates:\n",
    "    print (\"learning rate is: \" + str(i))\n",
    "    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, \n",
    "                           learning_rate = i, print_cost = False)\n",
    "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
    "\n",
    "for i in learning_rates:\n",
    "    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n",
    "\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations')\n",
    "\n",
    "legend = plt.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: \n",
    "- Different learning rates give different costs and thus different predictions results.\n",
    "- If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost). \n",
    "- A lower cost doesn't mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy.\n",
    "- In deep learning, we usually recommend that you: \n",
    "    - Choose the learning rate that better minimizes the cost function.\n",
    "    - If your model overfits, use other techniques to reduce overfitting. (We'll talk about this in later classes.) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Test your own image ##\n",
    "\n",
    "Congratulations on finishing this assignment. You can use your own image and see the output of your model. To do that:\n",
    "    1. Add your image to this Jupyter Notebook's directory, in the \"images\" folder\n",
    "    2. Change your image's name in the following code\n",
    "    3. Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-aa8cc5398420>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# We preprocess the image to fit your algorithm.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"images/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmy_image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Warning: In case issue with imread, please uninstall scipy and install scipy==1.1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'my_image' is not defined"
     ]
    }
   ],
   "source": [
    "## START CODE HERE ## (PUT YOUR IMAGE NAME) \n",
    "\n",
    "\n",
    "## END CODE HERE ##\n",
    "\n",
    "# We preprocess the image to fit your algorithm.\n",
    "fname = \"images/\" + my_image\n",
    "\n",
    "# Warning: In case issue with imread, please uninstall scipy and install scipy==1.1.0\n",
    "image = np.array(ndimage.imread(fname, flatten=False))\n",
    "\n",
    "my_image = scipy.misc.imresize(image, size=(num_px, num_px)).reshape((1, num_px * num_px * 3)).T\n",
    "my_predicted_image = predict(d[\"w\"], d[\"b\"], my_image)\n",
    "\n",
    "plt.imshow(image)\n",
    "print(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your algorithm predicts a \\\"\" \n",
    "      + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "What to remember from this assignment:\n",
    "1. Preprocessing the dataset is important.\n",
    "2. You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model().\n",
    "3. Tuning the learning rate (which is an example of a \"hyperparameter\") can make a big difference to the algorithm. You will see more examples of this later in this course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you'd like, we invite you to try different things on this Notebook. Make sure you submit before trying anything. Once you submit, things you can play with include:\n",
    "    - Play with the learning rate and the number of iterations\n",
    "    - Try different initialization methods and compare the results\n",
    "    - Test other preprocessings (center the data, or divide each row by its standard deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "- http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n",
    "- https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II [50 points]\n",
    "## 1. Support Vector Machines [20 points]\n",
    "\n",
    "In the first half of this exercise, you will be using support vector machines (SVMs) with various example 2D datasets. Experimenting with these datasets will help you gain an intuition of how SVMs work and how to use a Gaussian kernel with SVMs. In the next half of the exercise, you will be using support\n",
    "vector machines to build a spam classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages for coding part\n",
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Import regular expressions to process emails\n",
    "import re\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# will be used to load MATLAB mat datafile format\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# library written for this exercise providing additional functions for assignment submission, and others\n",
    "import utils\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Example Dataset 1\n",
    "\n",
    "We will begin by with a 2D example dataset which can be separated by a linear boundary. The following cell plots the training data, which should look like this:\n",
    "\n",
    "![Dataset 1 training data](figure/dataset1.png)\n",
    "\n",
    "In this dataset, the positions of the positive examples (indicated with `x`) and the negative examples (indicated with `o`) suggest a natural separation indicated by the gap. However, notice that there is an outlier positive example `x` on the far left at about (0.1, 4.1). As part of this exercise, you will also see how this outlier affects the SVM decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load from data1\n",
    "# You will have X, y as keys in the dict data\n",
    "data = loadmat(os.path.join('data', 'data1.mat'))\n",
    "X, y = data['X'], data['y'][:, 0]\n",
    "\n",
    "# Plot training data\n",
    "utils.plotData(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the exercise, you will try using different values of the $C$ parameter with SVMs. Informally, the $C$ parameter is a positive value that controls the penalty for misclassified training examples. A large $C$ parameter tells the SVM to try to classify all the examples correctly. $C$ plays a role similar to $1/\\lambda$, where $\\lambda$ is the regularization parameter that we were using previously for logistic regression.\n",
    "\n",
    "\n",
    "The following cell will run the SVM training (with $C=1$) using SVM software that we have included with the starter code (function `svmTrain` within the `utils` module of this exercise). When $C=1$, you should find that the SVM puts the decision boundary in the gap between the two datasets and *misclassifies* the data point on the far left, as shown in the figure (left) below.\n",
    "\n",
    "<table style=\"text-align:center\">\n",
    "    <tr>\n",
    "        <th colspan=\"2\" style=\"text-align:center\">SVM Decision boundary for example dataset 1 </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">C=1<img src=\"figure/svm_c1.png\"/></td>\n",
    "        <td style=\"text-align:center\">C=100<img src=\"figure/svm_c100.png\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "In order to minimize the dependency of this assignment on external libraries, we have included this implementation of an SVM learning algorithm in utils.svmTrain. However, this particular implementation is not very efficient (it was originally chosen to maximize compatibility between Octave/MATLAB for the first version of this assignment set). If you are training an SVM on a real problem, especially if you need to scale to a larger dataset, we strongly recommend instead using a highly optimized SVM toolbox such as [LIBSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/). The python machine learning library [scikit-learn](http://scikit-learn.org/stable/index.html) provides wrappers for the LIBSVM library.\n",
    "</div>\n",
    "<br/>\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Implementation Note: Most SVM software packages (including the function `utils.svmTrain`) automatically add the extra feature $x_0$ = 1 for you and automatically take care of learning the intercept term $\\theta_0$. So when passing your training data to the SVM software, there is no need to add this extra feature $x_0 = 1$ yourself. In particular, in python your code should be working with training examples $x \\in \\mathcal{R}^n$ (rather than $x \\in \\mathcal{R}^{n+1}$); for example, in the first example dataset $x \\in \\mathcal{R}^2$.\n",
    "</div>\n",
    "\n",
    "Your task is to try different values of $C$ on this dataset. Specifically, you should change the value of $C$ in the next cell to $C = 100$ and run the SVM training again. When $C = 100$, you should find that the SVM now classifies every single example correctly, but has a decision boundary that does not\n",
    "appear to be a natural fit for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You should try to change the C value below and see how the decision\n",
    "# boundary varies (e.g., try C = 1000)\n",
    "C = 100\n",
    "\n",
    "model = utils.svmTrain(X, y, C, utils.linearKernel, 1e-3, 20)\n",
    "utils.visualizeBoundaryLinear(X, y, model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"section1\"></a>\n",
    "### 1.2 SVM with Gaussian Kernels\n",
    "\n",
    "In this part of the exercise, you will be using SVMs to do non-linear classification. In particular, you will be using SVMs with Gaussian kernels on datasets that are not linearly separable.\n",
    "\n",
    "#### 1.2.1 Gaussian Kernel\n",
    "\n",
    "To find non-linear decision boundaries with the SVM, we need to first implement a Gaussian kernel. You can think of the Gaussian kernel as a similarity function that measures the “distance” between a pair of examples,\n",
    "($x^{(i)}$, $x^{(j)}$). The Gaussian kernel is also parameterized by a bandwidth parameter, $\\sigma$, which determines how fast the similarity metric decreases (to 0) as the examples are further apart.\n",
    "You should now complete the code in `gaussian_kernel` to compute the Gaussian kernel between two examples, ($x^{(i)}$, $x^{(j)}$). The Gaussian kernel function is defined as:\n",
    "\n",
    "$$ K_{\\text{gaussian}} \\left( x^{(i)}, x^{(j)} \\right) = \\exp \\left( - \\frac{\\left\\lvert\\left\\lvert x^{(i)} - x^{(j)}\\right\\lvert\\right\\lvert^2}{2\\sigma^2} \\right) = \\exp \\left( -\\frac{\\sum_{k=1}^n \\left( x_k^{(i)} - x_k^{(j)}\\right)^2}{2\\sigma^2} \\right)$$\n",
    "<a id=\"gaussianKernel\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x1, x2, sigma):\n",
    "    \"\"\"\n",
    "    Computes the radial basis function\n",
    "    Returns a radial basis function kernel between x1 and x2.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x1 :  numpy ndarray\n",
    "        A vector of size (n, ), representing the first datapoint.\n",
    "    \n",
    "    x2 : numpy ndarray\n",
    "        A vector of size (n, ), representing the second datapoint.\n",
    "    \n",
    "    sigma : float\n",
    "        The bandwidth parameter for the Gaussian kernel.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sim : float\n",
    "        The computed RBF between the two provided data points.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Fill in this function to return the similarity between `x1` and `x2`\n",
    "    computed using a Gaussian kernel with bandwidth `sigma`.\n",
    "    \"\"\"\n",
    "    sim = 0\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "    # =============================================================\n",
    "    return sim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have completed the function `gaussian_kernel` the following cell will test your kernel function on two provided examples and you should expect to see a value of 0.324652."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([1, 2, 1])\n",
    "x2 = np.array([0, 4, -1])\n",
    "sigma = 2\n",
    "\n",
    "sim = gaussian_kernel(x1, x2, sigma)\n",
    "\n",
    "print('Gaussian Kernel between x1 = [1, 2, 1], x2 = [0, 4, -1], sigma = %0.2f:'\n",
    "      '\\n\\t%f\\n(for sigma = 2, this value should be about 0.324652)\\n' % (sigma, sim))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Example Dataset 2\n",
    "\n",
    "The next part in this notebook will load and plot dataset 2, as shown in the figure below. \n",
    "\n",
    "![Dataset 2](figure/dataset2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from data2\n",
    "# You will have X, y as keys in the dict data\n",
    "data = loadmat(os.path.join('data', 'data2.mat'))\n",
    "X, y = data['X'], data['y'][:, 0]\n",
    "\n",
    "# Plot training data\n",
    "utils.plotData(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figure, you can obserse that there is no linear decision boundary that separates the positive and negative examples for this dataset. However, by using the Gaussian kernel with the SVM, you will be able to learn a non-linear decision boundary that can perform reasonably well for the dataset. If you have correctly implemented the Gaussian kernel function, the following cell will proceed to train the SVM with the Gaussian kernel on this dataset.\n",
    "\n",
    "You should get a decision boundary as shown in the figure below, as computed by the SVM with a Gaussian kernel. The decision boundary is able to separate most of the positive and negative examples correctly and follows the contours of the dataset well.\n",
    "\n",
    "![Dataset 2 decision boundary](figure/svm_dataset2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Parameters\n",
    "C = 1\n",
    "sigma = 0.1\n",
    "\n",
    "model= utils.svmTrain(X, y, C, gaussian_kernel, args=(sigma,))\n",
    "utils.visualizeBoundary(X, y, model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "#### 1.2.3 Example Dataset 3\n",
    "\n",
    "In this part of the exercise, you will gain more practical skills on how to use a SVM with a Gaussian kernel. The next cell will load and display a third dataset, which should look like the figure below.\n",
    "\n",
    "![Dataset 3](figure/dataset3.png)\n",
    "\n",
    "You will be using the SVM with the Gaussian kernel with this dataset. In the provided dataset, `hw4_data3.mat`, you are given the variables `X`, `y`, `Xval`, `yval`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from data3\n",
    "# You will have X, y, Xval, yval as keys in the dict data\n",
    "data = loadmat(os.path.join('data', 'data3.mat'))\n",
    "X, y, Xval, yval = data['X'], data['y'][:, 0], data['Xval'], data['yval'][:, 0]\n",
    "\n",
    "# Plot training data\n",
    "utils.plotData(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to use the cross validation set `Xval`, `yval` to determine the best $C$ and $\\sigma$ parameter to use. You should write any additional code necessary to help you search over the parameters $C$ and $\\sigma$. For both $C$ and $\\sigma$, we suggest trying values in multiplicative steps (e.g., 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30).\n",
    "Note that you should try all possible pairs of values for $C$ and $\\sigma$ (e.g., $C = 0.3$ and $\\sigma = 0.1$). For example, if you try each of the 8 values listed above for $C$ and for $\\sigma^2$, you would end up training and evaluating (on the cross validation set) a total of $8^2 = 64$ different models. After you have determined the best $C$ and $\\sigma$ parameters to use, you should modify the code in `dataset3_params`, filling in the best parameters you found. For our best parameters, the SVM returned a decision boundary shown in the figure below. \n",
    "\n",
    "![](figure/svm_dataset3_best.png)\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Implementation Tip: When implementing cross validation to select the best $C$ and $\\sigma$ parameter to use, you need to evaluate the error on the cross validation set. Recall that for classification, the error is defined as the fraction of the cross validation examples that were classified incorrectly. In `numpy`, you can compute this error using `np.mean(predictions != yval)`, where `predictions` is a vector containing all the predictions from the SVM, and `yval` are the true labels from the cross validation set. You can use the `utils.svmPredict` function to generate the predictions for the cross validation set.\n",
    "</div>\n",
    "<a id=\"dataset3Params\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset3_params(X, y, Xval, yval):\n",
    "    \"\"\"\n",
    "    Returns your choice of C and sigma for Part 3 (data3) of the exercise \n",
    "    where you select the optimal (C, sigma) learning parameters to use for SVM\n",
    "    with RBF kernel.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        (m x n) matrix of training data where m is number of training examples, and \n",
    "        n is the number of features.\n",
    "    \n",
    "    y : array_like\n",
    "        (m, ) vector of labels for ther training data.\n",
    "    \n",
    "    Xval : array_like\n",
    "        (mv x n) matrix of validation data where mv is the number of validation examples\n",
    "        and n is the number of features\n",
    "    \n",
    "    yval : array_like\n",
    "        (mv, ) vector of labels for the validation data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    C, sigma : float, float\n",
    "        The best performing values for the regularization parameter C and \n",
    "        RBF parameter sigma.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Fill in this function to return the optimal C and sigma learning \n",
    "    parameters found using the cross validation set.\n",
    "    You can use `svmPredict` to predict the labels on the cross\n",
    "    validation set. For example, \n",
    "    \n",
    "        predictions = svmPredict(model, Xval)\n",
    "\n",
    "    will return the predictions on the cross validation set.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    You can compute the prediction error using \n",
    "    \n",
    "        np.mean(predictions != yval)\n",
    "    \"\"\"\n",
    "    # You need to return the following variables correctly.\n",
    "    C = 1\n",
    "    sigma = 0.3\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "    \n",
    "    \n",
    "    # ============================================================\n",
    "    return C, sigma\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code in the next cell trains the SVM classifier using the training set $(X, y)$ using parameters loaded from `dataset3_params`. Note that this might take a few minutes to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different SVM Parameters here\n",
    "C, sigma = dataset3_params(X, y, Xval, yval)\n",
    "\n",
    "# Train the SVM\n",
    "# model = utils.svmTrain(X, y, C, lambda x1, x2: gaussian_kernel(x1, x2, sigma))\n",
    "model = utils.svmTrain(X, y, C, gaussian_kernel, args=(sigma,))\n",
    "utils.visualizeBoundary(X, y, model)\n",
    "print(C, sigma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2 Multi-class Classification [30 points]\n",
    "\n",
    "For this exercise, you will use logistic regression and neural networks to recognize handwritten digits (from 0 to 9). Automated handwritten digit recognition is widely used today - from recognizing zip codes (postal codes)\n",
    "on mail envelopes to recognizing amounts written on bank checks. This exercise will show you how the methods you have learned can be used for this classification task.\n",
    "\n",
    "In the first part of the exercise, you will extend your previous implementation of logistic regression and apply it to one-vs-all classification.\n",
    "\n",
    "### 2.1 Dataset\n",
    "\n",
    "You are given a data set in `MC_data1.mat` that contains 5000 training examples of handwritten digits (This is a subset of the [MNIST](http://yann.lecun.com/exdb/mnist) handwritten digit dataset). The `.mat` format means that that the data has been saved in a native Octave/MATLAB matrix format, instead of a text (ASCII) format like a csv-file. We use the `.mat` format here because this is the dataset provided in the MATLAB version of this assignment. Fortunately, python provides mechanisms to load MATLAB native format using the `loadmat` function within the `scipy.io` module. This function returns a python dictionary with keys containing the variable names within the `.mat` file. \n",
    "\n",
    "There are 5000 training examples in `MC_data1.mat`, where each training example is a 20 pixel by 20 pixel grayscale image of the digit. Each pixel is represented by a floating point number indicating the grayscale intensity at that location. The 20 by 20 grid of pixels is “unrolled” into a 400-dimensional vector. Each of these training examples becomes a single row in our data matrix `X`. This gives us a 5000 by 400 matrix `X` where every row is a training example for a handwritten digit image.\n",
    "\n",
    "$$ X = \\begin{bmatrix} - \\: (x^{(1)})^T \\: - \\\\ -\\: (x^{(2)})^T \\:- \\\\ \\vdots \\\\ - \\: (x^{(m)})^T \\:-  \\end{bmatrix} $$\n",
    "\n",
    "The second part of the training set is a 5000-dimensional vector `y` that contains labels for the training set. \n",
    "We start the exercise by first loading the dataset. Execute the cell below, you do not need to write any code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20x20 Input Images of Digits\n",
    "input_layer_size  = 400\n",
    "\n",
    "# 10 labels, from 1 to 10 (note that we have mapped \"0\" to label 10)\n",
    "num_labels = 10\n",
    "\n",
    "#  training data stored in arrays X, y\n",
    "data = loadmat(os.path.join('data', 'MC_data1.mat'))\n",
    "X, y = data['X'], data['y'].ravel()\n",
    "\n",
    "# set the zero digit to 0, rather than its mapped 10 in this dataset\n",
    "# This is an artifact due to the fact that this dataset was used in \n",
    "# MATLAB where there is no index 0\n",
    "y[y == 10] = 0\n",
    "\n",
    "m = y.size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualizing the data\n",
    "\n",
    "You will begin by visualizing a subset of the training set. In the following cell, the code randomly selects selects 100 rows from `X` and passes those rows to the `displayData` function. This function maps each row to a 20 pixel by 20 pixel grayscale image and displays the images together. We have provided the `displayData` function in the file `utils.py`. You are encouraged to examine the code to see how it works. Run the following cell to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 100 data points to display\n",
    "rand_indices = np.random.choice(m, 100, replace=False)\n",
    "sel = X[rand_indices, :]\n",
    "\n",
    "utils.displayData(sel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Vectorizing Logistic Regression\n",
    "\n",
    "You will be using multiple one-vs-all logistic regression models to build a multi-class classifier. Since there are 10 classes, you will need to train 10 separate logistic regression classifiers. To make this training efficient, it is important to ensure that your code is well vectorized. In this section, you will implement a vectorized version of logistic regression that does not employ any `for` loops. You can use your code in the previous exercise as a starting point for this exercise. \n",
    "\n",
    "To test your vectorized logistic regression, we will use custom data as defined in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test values for the parameters theta\n",
    "theta_t = np.array([-2, -1, 1, 2], dtype=float)\n",
    "\n",
    "# test values for the inputs\n",
    "X_t = np.concatenate([np.ones((5, 1)), np.arange(1, 16).reshape(5, 3, order='F')/10.0], axis=1)\n",
    "\n",
    "# test values for the labels\n",
    "y_t = np.array([1, 0, 1, 0, 1])\n",
    "\n",
    "# test value for the regularization parameter\n",
    "lambda_t = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "#### 2.3.1 Vectorizing the cost function \n",
    "\n",
    "We will begin by writing a vectorized version of the cost function. Recall that in (unregularized) logistic regression, the cost function is\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left[ -y^{(i)} \\log \\left( h_\\theta\\left( x^{(i)} \\right) \\right) - \\left(1 - y^{(i)} \\right) \\log \\left(1 - h_\\theta \\left( x^{(i)} \\right) \\right) \\right] $$\n",
    "\n",
    "To compute each element in the summation, we have to compute $h_\\theta(x^{(i)})$ for every example $i$, where $h_\\theta(x^{(i)}) = g(\\theta^T x^{(i)})$ and $g(z) = \\frac{1}{1+e^{-z}}$ is the sigmoid function. It turns out that we can compute this quickly for all our examples by using matrix multiplication. Let us define $X$ and $\\theta$ as\n",
    "\n",
    "$$ X = \\begin{bmatrix} - \\left( x^{(1)} \\right)^T - \\\\ - \\left( x^{(2)} \\right)^T - \\\\ \\vdots \\\\ - \\left( x^{(m)} \\right)^T - \\end{bmatrix} \\qquad \\text{and} \\qquad \\theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_n \\end{bmatrix} $$\n",
    "\n",
    "Then, by computing the matrix product $X\\theta$, we have: \n",
    "\n",
    "$$ X\\theta = \\begin{bmatrix} - \\left( x^{(1)} \\right)^T\\theta - \\\\ - \\left( x^{(2)} \\right)^T\\theta - \\\\ \\vdots \\\\ - \\left( x^{(m)} \\right)^T\\theta - \\end{bmatrix} = \\begin{bmatrix} - \\theta^T x^{(1)}  - \\\\ - \\theta^T x^{(2)} - \\\\ \\vdots \\\\ - \\theta^T x^{(m)}  - \\end{bmatrix} $$\n",
    "\n",
    "In the last equality, we used the fact that $a^Tb = b^Ta$ if $a$ and $b$ are vectors. This allows us to compute the products $\\theta^T x^{(i)}$ for all our examples $i$ in one line of code.\n",
    "\n",
    "#### 2.3.2 Vectorizing the gradient\n",
    "\n",
    "Recall that the gradient of the (unregularized) logistic regression cost is a vector where the $j^{th}$ element is defined as\n",
    "\n",
    "$$ \\frac{\\partial J }{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m \\left( \\left( h_\\theta\\left(x^{(i)}\\right) - y^{(i)} \\right)x_j^{(i)} \\right) $$\n",
    "\n",
    "To vectorize this operation over the dataset, we start by writing out all the partial derivatives explicitly for all $\\theta_j$,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial J}{\\partial \\theta_0} \\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_1} \\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_n}\n",
    "\\end{bmatrix} = &\n",
    "\\frac{1}{m} \\begin{bmatrix}\n",
    "\\sum_{i=1}^m \\left( \\left(h_\\theta\\left(x^{(i)}\\right) - y^{(i)} \\right)x_0^{(i)}\\right) \\\\\n",
    "\\sum_{i=1}^m \\left( \\left(h_\\theta\\left(x^{(i)}\\right) - y^{(i)} \\right)x_1^{(i)}\\right) \\\\\n",
    "\\sum_{i=1}^m \\left( \\left(h_\\theta\\left(x^{(i)}\\right) - y^{(i)} \\right)x_2^{(i)}\\right) \\\\\n",
    "\\vdots \\\\\n",
    "\\sum_{i=1}^m \\left( \\left(h_\\theta\\left(x^{(i)}\\right) - y^{(i)} \\right)x_n^{(i)}\\right) \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "= & \\frac{1}{m} \\sum_{i=1}^m \\left( \\left(h_\\theta\\left(x^{(i)}\\right) - y^{(i)} \\right)x^{(i)}\\right) \\\\\n",
    "= & \\frac{1}{m} X^T \\left( h_\\theta(x) - y\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$  h_\\theta(x) - y = \n",
    "\\begin{bmatrix}\n",
    "h_\\theta\\left(x^{(1)}\\right) - y^{(1)} \\\\\n",
    "h_\\theta\\left(x^{(2)}\\right) - y^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "h_\\theta\\left(x^{(m)}\\right) - y^{(m)} \n",
    "\\end{bmatrix} $$\n",
    "\n",
    "Note that $x^{(i)}$ is a vector, while $h_\\theta\\left(x^{(i)}\\right) - y^{(i)}$  is a scalar (single number).\n",
    "To understand the last step of the derivation, let $\\beta_i = (h_\\theta\\left(x^{(m)}\\right) - y^{(m)})$ and\n",
    "observe that:\n",
    "\n",
    "$$ \\sum_i \\beta_ix^{(i)} = \\begin{bmatrix} \n",
    "| & | & & | \\\\\n",
    "x^{(1)} & x^{(2)} & \\cdots & x^{(m)} \\\\\n",
    "| & | & & | \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\beta_1 \\\\\n",
    "\\beta_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_m\n",
    "\\end{bmatrix} = x^T \\beta\n",
    "$$\n",
    "\n",
    "where the values $\\beta_i = \\left( h_\\theta(x^{(i)} - y^{(i)} \\right)$.\n",
    "\n",
    "The expression above allows us to compute all the partial derivatives\n",
    "without any loops. If you are comfortable with linear algebra, we encourage you to work through the matrix multiplications above to convince yourself that the vectorized version does the same computations. \n",
    "\n",
    "Your job is to write the unregularized cost function `lr_cost_function` which returns both the cost function $J(\\theta)$ and its gradient $\\frac{\\partial J}{\\partial \\theta}$. Your implementation should use the strategy we presented above to calculate $\\theta^T x^{(i)}$. You should also use a vectorized approach for the rest of the cost function. A fully vectorized version of `lr_cost_function` should not contain any loops.\n",
    "\n",
    "<div class=\"alert alert-box alert-warning\">\n",
    "Debugging Tip: Vectorizing code can sometimes be tricky. One common strategy for debugging is to print out the sizes of the matrices you are working with using the `shape` property of `numpy` arrays. For example, given a data matrix $X$ of size $100 \\times 20$ (100 examples, 20 features) and $\\theta$, a vector with size $20$, you can observe that `np.dot(X, theta)` is a valid multiplication operation, while `np.dot(theta, X)` is not. Furthermore, if you have a non-vectorized version of your code, you can compare the output of your vectorized code and non-vectorized code to make sure that they produce the same outputs.\n",
    "</div>\n",
    "<a id=\"lrCostFunction\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_cost_function(theta, X, y, lambda_):\n",
    "    \"\"\"\n",
    "    Computes the cost of using theta as the parameter for regularized\n",
    "    logistic regression and the gradient of the cost w.r.t. to the parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : array_like\n",
    "        Logistic regression parameters. A vector with shape (n, ). n is \n",
    "        the number of features including any intercept.  \n",
    "    \n",
    "    X : array_like\n",
    "        The data set with shape (m x n). m is the number of examples, and\n",
    "        n is the number of features (including intercept).\n",
    "    \n",
    "    y : array_like\n",
    "        The data labels. A vector with shape (m, ).\n",
    "    \n",
    "    lambda_ : float\n",
    "        The regularization parameter. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed value for the regularized cost function. \n",
    "    \n",
    "    grad : array_like\n",
    "        A vector of shape (n, ) which is the gradient of the cost\n",
    "        function with respect to theta, at the current values of theta.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the cost of a particular choice of theta. You should set J to the cost.\n",
    "    Compute the partial derivatives and set grad to the partial\n",
    "    derivatives of the cost w.r.t. each parameter in theta\n",
    "    \n",
    "    Hint 1\n",
    "    ------\n",
    "    The computation of the cost function and gradients can be efficiently\n",
    "    vectorized. For example, consider the computation\n",
    "    \n",
    "        sigmoid(X * theta)\n",
    "    \n",
    "    Each row of the resulting matrix will contain the value of the prediction\n",
    "    for that example. You can make use of this to vectorize the cost function\n",
    "    and gradient computations. \n",
    "    \n",
    "    Hint 2\n",
    "    ------\n",
    "    When computing the gradient of the regularized cost function, there are\n",
    "    many possible vectorized solutions, but one solution looks like:\n",
    "    \n",
    "        grad = (unregularized gradient for logistic regression)\n",
    "        temp = theta \n",
    "        temp[0] = 0   # because we don't add anything for j = 0\n",
    "        grad = grad + YOUR_CODE_HERE (using the temp variable)\n",
    "    \n",
    "    Hint 3\n",
    "    ------\n",
    "    We have provided the implementatation of the sigmoid function within \n",
    "    the file `utils.py`. At the start of the notebook, we imported this file\n",
    "    as a module. Thus to access the sigmoid function within that file, you can\n",
    "    do the following: `utils.sigmoid(z)`.\n",
    "    \n",
    "    \"\"\"\n",
    "    #Initialize some useful values\n",
    "    m = y.size\n",
    "    \n",
    "    # convert labels to ints if their type is bool\n",
    "    if y.dtype == bool:\n",
    "        y = y.astype(int)\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    J = 0\n",
    "    grad = np.zeros(theta.shape)\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "\n",
    "        \n",
    "    # =============================================================\n",
    "    return J, grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Vectorizing regularized logistic regression\n",
    "\n",
    "After you have implemented vectorization for logistic regression, you will now\n",
    "add regularization to the cost function. Recall that for regularized logistic\n",
    "regression, the cost function is defined as\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left[ -y^{(i)} \\log \\left(h_\\theta\\left(x^{(i)} \\right)\\right) - \\left( 1 - y^{(i)} \\right) \\log\\left(1 - h_\\theta \\left(x^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2 $$\n",
    "\n",
    "Note that you should not be regularizing $\\theta_0$ which is used for the bias term.\n",
    "Correspondingly, the partial derivative of regularized logistic regression cost for $\\theta_j$ is defined as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta\\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)}  & \\text{for } j = 0 \\\\\n",
    "& \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\left( \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta\\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} \\right) + \\frac{\\lambda}{m} \\theta_j & \\text{for } j  \\ge 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now modify your code in `lr_cost_function` to account for regularization. Once again, you should not put any loops into your code.\n",
    "\n",
    "<div class=\"alert alert-box alert-warning\">\n",
    "python/numpy Tip: When implementing the vectorization for regularized logistic regression, you might often want to only sum and update certain elements of $\\theta$. In `numpy`, you can index into the matrices to access and update only certain elements. For example, A[:, 3:5]\n",
    "= B[:, 1:3] will replaces the columns with index 3 to 5 of A with the columns with index 1 to 3 from B. To select columns (or rows) until the end of the matrix, you can leave the right hand side of the colon blank. For example, A[:, 2:] will only return elements from the $3^{rd}$ to last columns of $A$. If you leave the left hand size of the colon blank, you will select elements from the beginning of the matrix. For example, A[:, :2] selects the first two columns, and is equivalent to A[:, 0:2]. In addition, you can use negative indices to index arrays from the end. Thus, A[:, :-1] selects all columns of A except the last column, and A[:, -5:] selects the $5^{th}$ column from the end to the last column. Thus, you could use this together with the sum and power ($^{**}$) operations to compute the sum of only the elements you are interested in (e.g., `np.sum(z[1:]**2)`). In the starter code, `lr_cost_function`, we have also provided hints on yet another possible method computing the regularized gradient.\n",
    "</div>\n",
    "\n",
    "Once you finished your implementation, you can call the function `lr_cost_function` to test your solution using the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J, grad = lr_cost_function(theta_t, X_t, y_t, lambda_t)\n",
    "\n",
    "print('Cost         : {:.6f}'.format(J))\n",
    "print('Expected cost: 2.534819')\n",
    "print('-----------------------')\n",
    "print('Gradients:')\n",
    "print(' [{:.6f}, {:.6f}, {:.6f}, {:.6f}]'.format(*grad))\n",
    "print('Expected gradients:')\n",
    "print(' [0.146561, -0.548558, 0.724722, 1.398003]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "### 2.4 One-vs-all Classification\n",
    "\n",
    "In this part of the exercise, you will implement one-vs-all classification by training multiple regularized logistic regression classifiers, one for each of the $K$ classes in our dataset. In the handwritten digits dataset, $K = 10$, but your code should work for any value of $K$. \n",
    "\n",
    "You should now complete the code for the function `one_vs_all` below, to train one classifier for each class. In particular, your code should return all the classifier parameters in a matrix $\\theta \\in \\mathbb{R}^{K \\times (N +1)}$, where each row of $\\theta$ corresponds to the learned logistic regression parameters for one class. You can do this with a “for”-loop from $0$ to $K-1$, training each classifier independently.\n",
    "\n",
    "Note that the `y` argument to this function is a vector of labels from 0 to 9. When training the classifier for class $k \\in \\{0, ..., K-1\\}$, you will want a K-dimensional vector of labels $y$, where $y_j \\in 0, 1$ indicates whether the $j^{th}$ training instance belongs to class $k$ $(y_j = 1)$, or if it belongs to a different\n",
    "class $(y_j = 0)$. You may find logical arrays helpful for this task. \n",
    "\n",
    "Furthermore, you will be using scipy's `optimize.minimize` for this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_vs_all(X, y, num_labels, lambda_):\n",
    "    \"\"\"\n",
    "    Trains num_labels logistic regression classifiers and returns\n",
    "    each of these classifiers in a matrix all_theta, where the i-th\n",
    "    row of all_theta corresponds to the classifier for label i.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The input dataset of shape (m x n). m is the number of \n",
    "        data points, and n is the number of features. Note that we \n",
    "        do not assume that the intercept term (or bias) is in X, however\n",
    "        we provide the code below to add the bias term to X. \n",
    "    \n",
    "    y : array_like\n",
    "        The data labels. A vector of shape (m, ).\n",
    "    \n",
    "    num_labels : int\n",
    "        Number of possible labels.\n",
    "    \n",
    "    lambda_ : float\n",
    "        The logistic regularization parameter.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    all_theta : array_like\n",
    "        The trained parameters for logistic regression for each class.\n",
    "        This is a matrix of shape (K x n+1) where K is number of classes\n",
    "        (ie. `numlabels`) and n is number of features without the bias.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    You should complete the following code to train `num_labels`\n",
    "    logistic regression classifiers with regularization parameter `lambda_`. \n",
    "    \n",
    "    Hint\n",
    "    ----\n",
    "    You can use y == c to obtain a vector of 1's and 0's that tell you\n",
    "    whether the ground truth is true/false for this class.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    For this assignment, we recommend using `scipy.optimize.minimize(method='CG')`\n",
    "    to optimize the cost function. It is okay to use a for-loop \n",
    "    (`for c in range(num_labels):`) to loop over the different classes.\n",
    "    \n",
    "    Example Code\n",
    "    ------------\n",
    "    \n",
    "        # Set Initial theta\n",
    "        initial_theta = np.zeros(n + 1)\n",
    "      \n",
    "        # Set options for minimize\n",
    "        options = {'maxiter': 50}\n",
    "    \n",
    "        # Run minimize to obtain the optimal theta. This function will \n",
    "        # return a class object where theta is in `res.x` and cost in `res.fun`\n",
    "        res = optimize.minimize(lrCostFunction, \n",
    "                                initial_theta, \n",
    "                                (X, (y == c), lambda_), \n",
    "                                jac=True, \n",
    "                                method='TNC',\n",
    "                                options=options) \n",
    "    \"\"\"\n",
    "    # Some useful variables\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # You need to return the following variables correctly \n",
    "    all_theta = np.zeros((num_labels, n + 1))\n",
    "\n",
    "    # Add ones to the X data matrix\n",
    "    X = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "   \n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    return all_theta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have completed the code for `one_vs_all`, the following cell will use your implementation to train a multi-class classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.1\n",
    "all_theta = oneVsAll(X, y, num_labels, lambda_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "#### 2.4.1 One-vs-all Prediction\n",
    "\n",
    "After training your one-vs-all classifier, you can now use it to predict the digit contained in a given image. For each input, you should compute the “probability” that it belongs to each class using the trained logistic regression classifiers. Your one-vs-all prediction function will pick the class for which the corresponding logistic regression classifier outputs the highest probability and return the class label (0, 1, ..., K-1) as the prediction for the input example. You should now complete the code in the function `predict_one_vs_all` to use the one-vs-all classifier for making predictions. \n",
    "<a id=\"predictOneVsAll\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_vs_all(all_theta, X):\n",
    "    \"\"\"\n",
    "    Return a vector of predictions for each example in the matrix X. \n",
    "    Note that X contains the examples in rows. all_theta is a matrix where\n",
    "    the i-th row is a trained logistic regression theta vector for the \n",
    "    i-th class. You should set p to a vector of values from 0..K-1 \n",
    "    (e.g., p = [0, 2, 0, 1] predicts classes 0, 2, 0, 1 for 4 examples) .\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    all_theta : array_like\n",
    "        The trained parameters for logistic regression for each class.\n",
    "        This is a matrix of shape (K x n+1) where K is number of classes\n",
    "        and n is number of features without the bias.\n",
    "    \n",
    "    X : array_like\n",
    "        Data points to predict their labels. This is a matrix of shape \n",
    "        (m x n) where m is number of data points to predict, and n is number \n",
    "        of features without the bias term. Note we add the bias term for X in \n",
    "        this function. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    p : array_like\n",
    "        The predictions for each data point in X. This is a vector of shape (m, ).\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Complete the following code to make predictions using your learned logistic\n",
    "    regression parameters (one-vs-all). You should set p to a vector of predictions\n",
    "    (from 0 to num_labels-1).\n",
    "    \n",
    "    Hint\n",
    "    ----\n",
    "    This code can be done all vectorized using the numpy argmax function.\n",
    "    In particular, the argmax function returns the index of the max element,\n",
    "    for more information see 'np.argmax' or search online. If your examples\n",
    "    are in rows, then, you can use np.argmax(A, axis=1) to obtain the index \n",
    "    of the max for each row.\n",
    "    \"\"\"\n",
    "    m = X.shape[0];\n",
    "    num_labels = all_theta.shape[0]\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    p = np.zeros(m)\n",
    "\n",
    "    # Add ones to the X data matrix\n",
    "    X = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "\n",
    "    \n",
    "    # ============================================================\n",
    "    return p\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are done, call your `predict_one_vs_all` function using the learned value of $\\theta$. You should see that the training set accuracy is about 95.1% (i.e., it classifies 95.1% of the examples in the training set correctly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_one_vs_all(all_theta, X)\n",
    "print('Training Set Accuracy: {:.2f}%'.format(np.mean(pred == y) * 100))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
